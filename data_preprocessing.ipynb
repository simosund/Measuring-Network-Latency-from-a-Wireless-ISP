{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f57f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import ipaddress\n",
    "import re\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from util import *\n",
    "from histogram_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d063a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common functions to the various parsing/merging functions\n",
    "def is_valid_date(datestr):\n",
    "    return re.match(\"^\\d{4}-\\d{2}-\\d{2}$\", datestr) is not None\n",
    "\n",
    "def is_epping_filename(filename):\n",
    "    return re.match(\"^pping\\..*\\.json.\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}[\\+\\-]\\d{2}:\\d{2}(\\.gz)?$\", \n",
    "                    filename) is not None\n",
    "\n",
    "def get_epping_files(root_folder):\n",
    "    files = []\n",
    "    for datedir in os.scandir(root_folder):\n",
    "        if not (datedir.is_dir() and is_valid_date(datedir.name)):\n",
    "            continue\n",
    "        \n",
    "        for gzfile in os.scandir(datedir.path):\n",
    "            if gzfile.is_file() and is_epping_filename(gzfile.name):\n",
    "                files.append(gzfile.path)\n",
    "    \n",
    "    return sorted(files)\n",
    "\n",
    "def classify_epping_entry_type(entry):\n",
    "    if \"aggregation_interval_ns\" in entry:\n",
    "        return \"specification\"\n",
    "    \n",
    "    if \"ip_prefix\" in entry:\n",
    "        return \"subnet_stats\"\n",
    "    \n",
    "    if \"protocol_counters\" in entry:\n",
    "        return \"global_counters\"\n",
    "    \n",
    "    return \"unknown\"\n",
    "\n",
    "def verify_epping_specification(json_agg_format, nbins=250, bin_width_ms=4, \n",
    "                                agg_interval_s=10, ipv4_prefix_len=24, \n",
    "                                ipv6_prefix_len=48):\n",
    "    fields = {\"bins\": nbins,\n",
    "              \"bin_width_ns\": bin_width_ms * 1e6,\n",
    "              \"aggregation_interval_ns\": agg_interval_s * 1e9,\n",
    "              \"ipv4_prefix_len\": ipv4_prefix_len,\n",
    "              \"ipv6_prefix_len\": ipv6_prefix_len}\n",
    "    \n",
    "    for field, expected in fields.items():\n",
    "        if json_agg_format[field] != expected:\n",
    "            raise ValueError(\"{} is {}, expected {}\".format(\n",
    "                field, json_agg_format[field], expected))\n",
    "    return True\n",
    "\n",
    "def get_epping_json_entries(filename, verify_specification=True, **kwargs):\n",
    "    with gzip.open(filename) as jfile:\n",
    "        entries = json.load(jfile)\n",
    "    \n",
    "    if not classify_epping_entry_type(entries[0]) == \"specification\":\n",
    "        raise ValueError(\"{} does not start with a specification entry\".format(filename))\n",
    "    \n",
    "    for entry in entries[1:]:\n",
    "        if classify_epping_entry_type(entry) == \"specification\":\n",
    "            raise ValueError(\"{} contains multiple specification entires\".format(filename))\n",
    "    \n",
    "    if verify_specification:\n",
    "        verify_epping_specification(entries[0], **kwargs)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def timestamp_to_datetime(timestamps, timezone=None):\n",
    "    dt = pd.to_datetime(timestamps, unit=\"ns\", utc=True)\n",
    "    if timezone is not None:\n",
    "        dt = dt.tz_convert(timezone)\n",
    "    \n",
    "    return dt\n",
    "\n",
    "def datetime_to_timestamp(dt, origin=np.datetime64(\"1970-01-01T00:00:00\"), unit=\"ns\"):\n",
    "    if isinstance(dt, str):\n",
    "        dt = pd.to_datetime(dt)\n",
    "    \n",
    "    if hasattr(dt, \"tz\") and dt.tz is not None:\n",
    "        dt = dt.tz_convert(\"UTC\")\n",
    "        dt = dt.tz_localize(None)\n",
    "    \n",
    "    return (dt - origin) // np.timedelta64(1, unit)\n",
    "\n",
    "def is_in_subnets(subnet, include_subnets):\n",
    "    subnet = netify(subnet)\n",
    "    include_subnets = [netify(net) for net in include_subnets]\n",
    "    \n",
    "    for net in include_subnets:\n",
    "        if subnet.version == net.version and subnet.subnet_of(net):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _get_subnet_type(subnet, lan_subnets=None):\n",
    "    if lan_subnets is None:\n",
    "        if subnet.is_private:\n",
    "            return \"LAN\"\n",
    "    else:\n",
    "        for lan in lan_subnets:\n",
    "            if subnet.version == lan.version and subnet.subnet_of(lan):\n",
    "                return \"LAN\"\n",
    "    \n",
    "    return \"WAN\" if subnet.is_global else \"suspicious\"\n",
    "\n",
    "def _new_subnet_entry(entry, subnet_enum_state, lan_subnets, histogram_bins):\n",
    "    subnet = ipaddress.ip_network(entry[\"ip_prefix\"])\n",
    "    new_entry = {\"ip_version\": subnet.version, \n",
    "                 \"prefix_length\": subnet.prefixlen,\n",
    "                 \"subnet_num\": subnet_enum_state[\"next_num\"],\n",
    "                 \"type\": _get_subnet_type(subnet, lan_subnets),\n",
    "                 \"reports\": 1, \n",
    "                 \"first_instance\": entry[\"timestamp\"],\n",
    "                 \"last_instance\": entry[\"timestamp\"]}\n",
    "    \n",
    "    subnet_enum_state[\"next_num\"] += 1\n",
    "    \n",
    "    new_entry[\"rx_stats\"] = copy.deepcopy(entry[\"rx_stats\"])\n",
    "    new_entry[\"tx_stats\"] = copy.deepcopy(entry[\"tx_stats\"])\n",
    "    \n",
    "    new_entry[\"rtt_min\"] = entry.get(\"min_rtt\")\n",
    "    new_entry[\"rtt_max\"] = entry.get(\"max_rtt\")\n",
    "    \n",
    "    new_entry[\"rtt_hist\"] = entry.get(\"histogram\", [])\n",
    "    new_entry[\"rtt_hist\"] += [0] * (histogram_bins - len(new_entry[\"rtt_hist\"]))\n",
    "    \n",
    "    return new_entry\n",
    "\n",
    "def _update_subnet_entry(tot, new_stats):\n",
    "    tot[\"reports\"] += 1\n",
    "    \n",
    "    t = new_stats[\"timestamp\"]\n",
    "    if t < tot[\"first_instance\"]:\n",
    "        tot[\"first_instance\"] = t\n",
    "    elif t > tot[\"last_instance\"]:\n",
    "        tot[\"last_instance\"] = t\n",
    "    \n",
    "    _update_rxtx_ttype_counters(tot[\"rx_stats\"], new_stats[\"rx_stats\"])\n",
    "    _update_rxtx_ttype_counters(tot[\"tx_stats\"], new_stats[\"tx_stats\"])\n",
    "    \n",
    "    if \"min_rtt\" in new_stats and \"max_rtt\" in new_stats:\n",
    "        if tot[\"rtt_min\"] is None or new_stats[\"min_rtt\"] < tot[\"rtt_min\"]:\n",
    "            tot[\"rtt_min\"] = new_stats[\"min_rtt\"]\n",
    "        if tot[\"rtt_max\"] is None or new_stats[\"max_rtt\"] > tot[\"rtt_max\"]:\n",
    "            tot[\"rtt_max\"] = new_stats[\"max_rtt\"]\n",
    "    \n",
    "    if \"histogram\" in new_stats:\n",
    "        add_to_histogram(tot[\"rtt_hist\"], new_stats[\"histogram\"])\n",
    "\n",
    "def _update_rxtx_ttype_counters(to_stats, from_stats):\n",
    "    for ttype in from_stats.keys():\n",
    "        \n",
    "        if ttype in to_stats:\n",
    "            for counter in from_stats[ttype]:\n",
    "                to_stats[ttype][counter] += from_stats[ttype][counter]\n",
    "        else:\n",
    "            to_stats[ttype] = copy.deepcopy(from_stats[ttype])\n",
    "    \n",
    "    return to_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1af66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d38b9708",
   "metadata": {},
   "source": [
    "### Merge all traffic per subnet (ignore time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7041e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_total_subnet_stats(root_folder, specification_kwargs={}, lan_subnets=[], report_freq=1440):\n",
    "    subnet_stats = dict()\n",
    "    subnet_enum_state = {\"next_num\": 0}\n",
    "    \n",
    "    nfiles = 0\n",
    "    nentries = 0\n",
    "    \n",
    "    lan_subnets = [ipaddress.ip_network(lan) for lan in lan_subnets]\n",
    "    files = get_epping_files(root_folder)\n",
    "    \n",
    "    for file in files:\n",
    "        entries = get_epping_json_entries(file, verify_specification=True, \n",
    "                                          **specification_kwargs)\n",
    "        \n",
    "        nbins = entries[0][\"bins\"]\n",
    "        \n",
    "        for entry in entries:\n",
    "            if classify_epping_entry_type(entry) != \"subnet_stats\":\n",
    "                continue\n",
    "            \n",
    "            subnet = entry[\"ip_prefix\"]\n",
    "            if subnet in subnet_stats:\n",
    "                _update_subnet_entry(subnet_stats[subnet], entry)\n",
    "            else:\n",
    "                subnet_stats[subnet] = _new_subnet_entry(entry, subnet_enum_state, \n",
    "                                                         lan_subnets, nbins)\n",
    "            \n",
    "            nentries += 1\n",
    "        \n",
    "        nfiles += 1\n",
    "        \n",
    "        if (report_freq > 0):\n",
    "            if (nfiles % report_freq == 0 or nfiles == len(files)):\n",
    "                print(\"{}: Parsed {}/{} files, containing {} entries\".format(\n",
    "                    datetime.datetime.now(), nfiles, len(files), nentries))\n",
    "    \n",
    "    return subnet_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec15be43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22 10:11:32.025234: Parsed 1440/46522 files, containing 10467910 entries\n",
      "2024-08-22 10:14:33.898976: Parsed 2880/46522 files, containing 20668102 entries\n",
      "2024-08-22 10:17:55.285506: Parsed 4320/46522 files, containing 31254943 entries\n",
      "2024-08-22 10:21:11.958352: Parsed 5760/46522 files, containing 41560292 entries\n",
      "2024-08-22 10:24:39.517049: Parsed 7200/46522 files, containing 52155260 entries\n",
      "2024-08-22 10:28:19.232153: Parsed 8640/46522 files, containing 63159910 entries\n",
      "2024-08-22 10:32:06.270317: Parsed 10080/46522 files, containing 74321577 entries\n",
      "2024-08-22 10:35:46.670234: Parsed 11520/46522 files, containing 85301424 entries\n",
      "2024-08-22 10:39:22.196905: Parsed 12960/46522 files, containing 95902501 entries\n",
      "2024-08-22 10:42:29.206985: Parsed 14400/46522 files, containing 104634489 entries\n",
      "2024-08-22 10:46:16.228337: Parsed 15840/46522 files, containing 115385861 entries\n",
      "2024-08-22 10:50:12.228419: Parsed 17280/46522 files, containing 126557607 entries\n",
      "2024-08-22 10:54:04.702053: Parsed 18720/46522 files, containing 137467712 entries\n",
      "2024-08-22 10:57:59.791238: Parsed 20160/46522 files, containing 148762448 entries\n",
      "2024-08-22 11:01:51.066575: Parsed 21600/46522 files, containing 159741325 entries\n",
      "2024-08-22 11:05:44.837180: Parsed 23040/46522 files, containing 170480252 entries\n",
      "2024-08-22 11:09:29.437872: Parsed 24480/46522 files, containing 180867044 entries\n",
      "2024-08-22 11:13:26.320446: Parsed 25920/46522 files, containing 191525362 entries\n",
      "2024-08-22 11:17:12.176219: Parsed 27360/46522 files, containing 201965138 entries\n",
      "2024-08-22 11:21:08.848220: Parsed 28800/46522 files, containing 212906984 entries\n",
      "2024-08-22 11:25:17.679328: Parsed 30240/46522 files, containing 224237508 entries\n",
      "2024-08-22 11:29:26.217280: Parsed 31680/46522 files, containing 235616157 entries\n",
      "2024-08-22 11:33:42.825580: Parsed 33120/46522 files, containing 246877252 entries\n",
      "2024-08-22 11:37:46.054231: Parsed 34560/46522 files, containing 258027615 entries\n",
      "2024-08-22 11:41:50.425002: Parsed 36000/46522 files, containing 269138074 entries\n",
      "2024-08-22 11:46:07.058483: Parsed 37440/46522 files, containing 280706409 entries\n",
      "2024-08-22 11:50:16.767909: Parsed 38880/46522 files, containing 291869366 entries\n",
      "2024-08-22 11:54:33.632598: Parsed 40320/46522 files, containing 303342717 entries\n",
      "2024-08-22 11:58:41.071709: Parsed 41760/46522 files, containing 314548216 entries\n",
      "2024-08-22 12:02:44.517264: Parsed 43200/46522 files, containing 325673493 entries\n",
      "2024-08-22 12:06:41.064939: Parsed 44640/46522 files, containing 336270917 entries\n",
      "2024-08-22 12:10:46.862046: Parsed 46080/46522 files, containing 347246444 entries\n",
      "2024-08-22 12:11:58.766076: Parsed 46522/46522 files, containing 350370438 entries\n",
      "CPU times: user 2h 2min 29s, sys: 11.5 s, total: 2h 2min 41s\n",
      "Wall time: 2h 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lan_subnets = (\"100.64.0.0/24\",\n",
    "               \"2602:fdca:800::/48\")\n",
    "\n",
    "subnet_stats = parse_total_subnet_stats(\"data/raw\", lan_subnets=lan_subnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5100d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.3 s, sys: 172 ms, total: 28.5 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open(\"data/processed/subnet_stats_merged.json\", \"wt\") as ofile:\n",
    "    json.dump(subnet_stats, ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73382a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae05d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b972673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afdf3e73",
   "metadata": {},
   "source": [
    "### Merge specific subnets with specific time resolution (ignore separate subnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6a85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_subnets_over_time(root_folder, include_subnets=None, exclude_subnets=None, \n",
    "                            aggfreq_s=None, tz=None, specification_kwargs={}, report_freq=1440):\n",
    "    time_stats = dict()\n",
    "    subnet_enum_state = {\"next_num\": 0}\n",
    "    \n",
    "    nfiles = 0\n",
    "    nentries = 0\n",
    "    \n",
    "    if include_subnets is not None:\n",
    "        include_subnets = [netify(net) for net in include_subnets]\n",
    "    if exclude_subnets is not None:\n",
    "        exclude_subnets = [netify(net) for net in exclude_subnets]\n",
    "    \n",
    "    files = get_epping_files(root_folder)\n",
    "    \n",
    "    for file in files:\n",
    "        entries = get_epping_json_entries(file, verify_specification=True, \n",
    "                                          **specification_kwargs)\n",
    "        \n",
    "        nbins = entries[0][\"bins\"]\n",
    "        \n",
    "        for entry in entries:\n",
    "            if classify_epping_entry_type(entry) != \"subnet_stats\":\n",
    "                continue\n",
    "            \n",
    "            subnet = netify(entry[\"ip_prefix\"])\n",
    "            if (include_subnets is not None and \n",
    "                not is_in_subnets(subnet, include_subnets)):\n",
    "                continue\n",
    "            if (exclude_subnets is not None and \n",
    "                is_in_subnets(subnet, exclude_subnets)):\n",
    "                continue\n",
    "            \n",
    "            t = entry[\"timestamp\"]\n",
    "            if aggfreq_s is not None:\n",
    "                t = int_ceil(t, aggfreq_s * 1000000000)\n",
    "            \n",
    "            if t in time_stats:\n",
    "                _update_subnet_entry(time_stats[t], entry)\n",
    "            else:\n",
    "                time_stats[t] = _new_subnet_entry(entry, subnet_enum_state, \n",
    "                                                  None, nbins)\n",
    "            \n",
    "            nentries += 1\n",
    "        \n",
    "        nfiles += 1\n",
    "        \n",
    "        if (report_freq > 0):\n",
    "            if (nfiles % report_freq == 0 or nfiles == len(files)):\n",
    "                print(\"{}: Parsed {}/{} files, containing {} entries\".format(\n",
    "                    datetime.datetime.now(), nfiles, len(files), nentries))\n",
    "    \n",
    "    return time_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00234a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22 12:16:54.768261: Parsed 1440/46522 files, containing 17282 entries\n",
      "2024-08-22 12:21:13.436362: Parsed 2880/46522 files, containing 34568 entries\n",
      "2024-08-22 12:25:42.524904: Parsed 4320/46522 files, containing 51854 entries\n",
      "2024-08-22 12:30:09.234223: Parsed 5760/46522 files, containing 69140 entries\n",
      "2024-08-22 12:34:36.515740: Parsed 7200/46522 files, containing 86426 entries\n",
      "2024-08-22 12:39:20.269778: Parsed 8640/46522 files, containing 103712 entries\n",
      "2024-08-22 12:44:08.696613: Parsed 10080/46522 files, containing 120998 entries\n",
      "2024-08-22 12:48:51.233737: Parsed 11520/46522 files, containing 138284 entries\n",
      "2024-08-22 12:53:15.168423: Parsed 12960/46522 files, containing 155097 entries\n",
      "2024-08-22 12:56:55.584750: Parsed 14400/46522 files, containing 168985 entries\n",
      "2024-08-22 13:01:22.258349: Parsed 15840/46522 files, containing 186271 entries\n",
      "2024-08-22 13:05:58.685243: Parsed 17280/46522 files, containing 203555 entries\n",
      "2024-08-22 13:10:30.360365: Parsed 18720/46522 files, containing 220841 entries\n",
      "2024-08-22 13:15:11.468374: Parsed 20160/46522 files, containing 238127 entries\n",
      "2024-08-22 13:19:45.258876: Parsed 21600/46522 files, containing 255413 entries\n",
      "2024-08-22 13:24:12.721238: Parsed 23040/46522 files, containing 272653 entries\n",
      "2024-08-22 13:28:33.586698: Parsed 24480/46522 files, containing 289939 entries\n",
      "2024-08-22 13:33:11.774127: Parsed 25920/46522 files, containing 307155 entries\n",
      "2024-08-22 13:37:43.348979: Parsed 27360/46522 files, containing 324442 entries\n",
      "2024-08-22 13:42:26.810336: Parsed 28800/46522 files, containing 341728 entries\n",
      "2024-08-22 13:47:23.989517: Parsed 30240/46522 files, containing 359014 entries\n",
      "2024-08-22 13:52:20.075624: Parsed 31680/46522 files, containing 376300 entries\n",
      "2024-08-22 13:57:09.598419: Parsed 33120/46522 files, containing 393562 entries\n",
      "2024-08-22 14:01:57.938085: Parsed 34560/46522 files, containing 410848 entries\n",
      "2024-08-22 14:06:45.744044: Parsed 36000/46522 files, containing 428134 entries\n",
      "2024-08-22 14:11:54.552253: Parsed 37440/46522 files, containing 445420 entries\n",
      "2024-08-22 14:16:54.057201: Parsed 38880/46522 files, containing 462688 entries\n",
      "2024-08-22 14:21:59.287464: Parsed 40320/46522 files, containing 479974 entries\n",
      "2024-08-22 14:26:46.919201: Parsed 41760/46522 files, containing 497260 entries\n",
      "2024-08-22 14:31:41.053417: Parsed 43200/46522 files, containing 514546 entries\n",
      "2024-08-22 14:36:20.800319: Parsed 44640/46522 files, containing 531756 entries\n",
      "2024-08-22 14:41:05.536896: Parsed 46080/46522 files, containing 549018 entries\n",
      "2024-08-22 14:42:25.258922: Parsed 46522/46522 files, containing 554324 entries\n",
      "CPU times: user 2h 29min 24s, sys: 10.5 s, total: 2h 29min 34s\n",
      "Wall time: 2h 29min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "target_subnets = (\"100.64.0.0/24\",\n",
    "                  \"2602:fdca:800::/48\")\n",
    "\n",
    "lan_per10s_stats = parse_subnets_over_time(\"data/raw/\", \n",
    "                                           include_subnets=target_subnets, \n",
    "                                           aggfreq_s=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62e2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/lan_per10s_stats_merged.json\", \"wt\") as ofile:\n",
    "        json.dump(lan_per10s_stats, ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c73cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d68ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22 14:48:40.620533: Parsed 1440/46522 files, containing 10450628 entries\n",
      "2024-08-22 14:53:54.472250: Parsed 2880/46522 files, containing 20633534 entries\n",
      "2024-08-22 14:59:20.898017: Parsed 4320/46522 files, containing 31203089 entries\n",
      "2024-08-22 15:04:32.941195: Parsed 5760/46522 files, containing 41491152 entries\n",
      "2024-08-22 15:09:55.742156: Parsed 7200/46522 files, containing 52068834 entries\n",
      "2024-08-22 15:15:25.222686: Parsed 8640/46522 files, containing 63056198 entries\n",
      "2024-08-22 15:21:06.889845: Parsed 10080/46522 files, containing 74200579 entries\n",
      "2024-08-22 15:26:42.186120: Parsed 11520/46522 files, containing 85163140 entries\n",
      "2024-08-22 15:32:03.225903: Parsed 12960/46522 files, containing 95747404 entries\n",
      "2024-08-22 15:36:33.235220: Parsed 14400/46522 files, containing 104465504 entries\n",
      "2024-08-22 15:42:01.671108: Parsed 15840/46522 files, containing 115199590 entries\n",
      "2024-08-22 15:47:40.985922: Parsed 17280/46522 files, containing 126354052 entries\n",
      "2024-08-22 15:53:24.713668: Parsed 18720/46522 files, containing 137246871 entries\n",
      "2024-08-22 15:59:12.895502: Parsed 20160/46522 files, containing 148524321 entries\n",
      "2024-08-22 16:04:56.122847: Parsed 21600/46522 files, containing 159485912 entries\n",
      "2024-08-22 16:10:32.100109: Parsed 23040/46522 files, containing 170207599 entries\n",
      "2024-08-22 16:15:50.646500: Parsed 24480/46522 files, containing 180577105 entries\n",
      "2024-08-22 16:21:23.291715: Parsed 25920/46522 files, containing 191218207 entries\n",
      "2024-08-22 16:26:47.357953: Parsed 27360/46522 files, containing 201640696 entries\n",
      "2024-08-22 16:32:24.244950: Parsed 28800/46522 files, containing 212565256 entries\n",
      "2024-08-22 16:38:14.874857: Parsed 30240/46522 files, containing 223878494 entries\n",
      "2024-08-22 16:43:59.344835: Parsed 31680/46522 files, containing 235239857 entries\n",
      "2024-08-22 16:49:40.990781: Parsed 33120/46522 files, containing 246483690 entries\n",
      "2024-08-22 16:55:18.139586: Parsed 34560/46522 files, containing 257616767 entries\n",
      "2024-08-22 17:00:48.389848: Parsed 36000/46522 files, containing 268709940 entries\n",
      "2024-08-22 17:06:36.053425: Parsed 37440/46522 files, containing 280260989 entries\n",
      "2024-08-22 17:12:15.114463: Parsed 38880/46522 files, containing 291406678 entries\n",
      "2024-08-22 17:18:02.448297: Parsed 40320/46522 files, containing 302862743 entries\n",
      "2024-08-22 17:23:44.152627: Parsed 41760/46522 files, containing 314050956 entries\n",
      "2024-08-22 17:29:20.405460: Parsed 43200/46522 files, containing 325158947 entries\n",
      "2024-08-22 17:34:46.740216: Parsed 44640/46522 files, containing 335739161 entries\n",
      "2024-08-22 17:40:21.980024: Parsed 46080/46522 files, containing 346697426 entries\n",
      "2024-08-22 17:41:54.864475: Parsed 46522/46522 files, containing 349816114 entries\n",
      "CPU times: user 2h 58min 27s, sys: 10.2 s, total: 2h 58min 37s\n",
      "Wall time: 2h 58min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "exclude_subnets = (\"100.64.0.0/24\",\n",
    "                  \"2602:fdca:800::/48\")\n",
    "\n",
    "wan_per10s_stats = parse_subnets_over_time(\"data/raw/\", \n",
    "                                           exclude_subnets=exclude_subnets, \n",
    "                                           aggfreq_s=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c901bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/wan_per10s_stats_merged.json\", \"wt\") as ofile:\n",
    "    json.dump(wan_per10s_stats, ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293c2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a409c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4baa43c4",
   "metadata": {},
   "source": [
    "### Parse all global counters (protocol, ECN and errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3f604bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_global_counters(root_folder, specification_kwargs={}, report_freq=1440, tz=None):\n",
    "    proto_entries = []\n",
    "    ecn_entries = []\n",
    "    error_entries = []\n",
    "    \n",
    "    nfiles = 0\n",
    "    nentries = 0\n",
    "    \n",
    "    \n",
    "    files = get_epping_files(root_folder)\n",
    "    for file in files:\n",
    "        entries = get_epping_json_entries(file, verify_specification=True, \n",
    "                                          **specification_kwargs)\n",
    "        \n",
    "        for entry in entries:\n",
    "            if classify_epping_entry_type(entry) != \"global_counters\":\n",
    "                continue\n",
    "            \n",
    "            t = entry[\"timestamp\"]\n",
    "            \n",
    "            if len(entry[\"protocol_counters\"]) > 0:\n",
    "                proto_entries.append(parse_protocol_counters(t, entry[\"protocol_counters\"]))\n",
    "            \n",
    "            if len(entry[\"ecn_counters\"]) > 0:\n",
    "                ecn_entries.append(parse_ecn_counters(t, entry[\"ecn_counters\"]))\n",
    "            \n",
    "            if len(entry[\"errors\"]) > 0:\n",
    "                error_entries.append(parse_error_counters(t, entry[\"errors\"]))\n",
    "            \n",
    "            nentries += 1\n",
    "        \n",
    "        nfiles += 1\n",
    "        \n",
    "        if (report_freq > 0):\n",
    "            if (nfiles % report_freq == 0 or nfiles == len(files)):\n",
    "                print(\"{}: Parsed {}/{} files, containing {} entries\".format(\n",
    "                    datetime.datetime.now(), nfiles, len(files), nentries))\n",
    "    \n",
    "    proto_stats = _records_to_dataframe(proto_entries, tz)\n",
    "    ecn_stats = _records_to_dataframe(ecn_entries, tz)\n",
    "    error_stats = _records_to_dataframe(error_entries, tz)\n",
    "    \n",
    "    return proto_stats, ecn_stats, error_stats\n",
    "\n",
    "def _records_to_dataframe(stats_records, tz=None):\n",
    "    df = pd.DataFrame.from_records(stats_records)\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    df[\"timestamp\"] = timestamp_to_datetime(df[\"timestamp\"].values, timezone=tz)\n",
    "    df.fillna(value=0, inplace=True)\n",
    "    for col in df.columns:\n",
    "        if \"float\" in str(df[col].dtype):\n",
    "            df[col] = df[col].astype(\"int64\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_protocol_counters(timestamp, protocol_counters):\n",
    "    counters = {\"timestamp\": timestamp}\n",
    "    \n",
    "    for protocol in protocol_counters.keys():\n",
    "        for counter, val in protocol_counters[protocol].items():\n",
    "            counter_key = \"{}_{}\".format(protocol, counter)\n",
    "            counters[counter_key] = val\n",
    "    \n",
    "    return counters\n",
    "\n",
    "def parse_ecn_counters(timestamp, ecn_counters):\n",
    "    counters = {\"timestamp\": timestamp}\n",
    "    counters.update(ecn_counters)\n",
    "    return counters\n",
    "\n",
    "def parse_error_counters(timestamp, error_counters):\n",
    "    counters = {\"timestamp\": timestamp}\n",
    "    counters.update(error_counters)\n",
    "    return counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5b387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103b0697",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22 17:45:45.033902: Parsed 1440/46522 files, containing 8641 entries\n",
      "2024-08-22 17:48:44.688689: Parsed 2880/46522 files, containing 17284 entries\n",
      "2024-08-22 17:51:49.276783: Parsed 4320/46522 files, containing 25927 entries\n",
      "2024-08-22 17:54:49.044725: Parsed 5760/46522 files, containing 34570 entries\n",
      "2024-08-22 17:57:52.097774: Parsed 7200/46522 files, containing 43213 entries\n",
      "2024-08-22 18:01:02.458347: Parsed 8640/46522 files, containing 51856 entries\n",
      "2024-08-22 18:04:20.532765: Parsed 10080/46522 files, containing 60499 entries\n",
      "2024-08-22 18:07:30.200843: Parsed 11520/46522 files, containing 69142 entries\n",
      "2024-08-22 18:10:35.574635: Parsed 12960/46522 files, containing 77784 entries\n",
      "2024-08-22 18:13:10.416174: Parsed 14400/46522 files, containing 86416 entries\n",
      "2024-08-22 18:16:17.474311: Parsed 15840/46522 files, containing 95059 entries\n",
      "2024-08-22 18:19:36.532610: Parsed 17280/46522 files, containing 103701 entries\n",
      "2024-08-22 18:22:47.463154: Parsed 18720/46522 files, containing 112344 entries\n",
      "2024-08-22 18:26:10.582364: Parsed 20160/46522 files, containing 120987 entries\n",
      "2024-08-22 18:29:27.099487: Parsed 21600/46522 files, containing 129630 entries\n",
      "2024-08-22 18:32:37.825363: Parsed 23040/46522 files, containing 138250 entries\n",
      "2024-08-22 18:35:47.774460: Parsed 24480/46522 files, containing 146893 entries\n",
      "2024-08-22 18:38:59.471890: Parsed 25920/46522 files, containing 155501 entries\n",
      "2024-08-22 18:41:56.513952: Parsed 27360/46522 files, containing 164144 entries\n",
      "2024-08-22 18:45:02.412658: Parsed 28800/46522 files, containing 172787 entries\n",
      "2024-08-22 18:48:15.162193: Parsed 30240/46522 files, containing 181430 entries\n",
      "2024-08-22 18:51:28.425592: Parsed 31680/46522 files, containing 190073 entries\n",
      "2024-08-22 18:54:40.680447: Parsed 33120/46522 files, containing 198704 entries\n",
      "2024-08-22 18:57:47.439933: Parsed 34560/46522 files, containing 207347 entries\n",
      "2024-08-22 19:00:58.154128: Parsed 36000/46522 files, containing 215990 entries\n",
      "2024-08-22 19:04:13.410322: Parsed 37440/46522 files, containing 224633 entries\n",
      "2024-08-22 19:07:24.789779: Parsed 38880/46522 files, containing 233267 entries\n",
      "2024-08-22 19:10:41.097664: Parsed 40320/46522 files, containing 241910 entries\n",
      "2024-08-22 19:13:48.531688: Parsed 41760/46522 files, containing 250553 entries\n",
      "2024-08-22 19:17:01.545263: Parsed 43200/46522 files, containing 259196 entries\n",
      "2024-08-22 19:20:03.745316: Parsed 44640/46522 files, containing 267803 entries\n",
      "2024-08-22 19:23:10.716624: Parsed 46080/46522 files, containing 276434 entries\n",
      "2024-08-22 19:24:02.557583: Parsed 46522/46522 files, containing 279087 entries\n",
      "CPU times: user 1h 41min 20s, sys: 9.68 s, total: 1h 41min 30s\n",
      "Wall time: 1h 41min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "proto, ecn, error = parse_global_counters(\"data/raw/\", tz=\"US/Mountain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19ce2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "proto.to_hdf(\"data/processed/protocol_counters.hdf5\", \"data\", \n",
    "             complevel=9)\n",
    "ecn.to_hdf(\"data/processed/ecn_counters.hdf5\", \"data\", \n",
    "           complevel=9)\n",
    "error.to_hdf(\"data/processed/error_counters.hdf5\", \"data\", \n",
    "             complevel=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c5e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2ac86e4",
   "metadata": {},
   "source": [
    "### Merge per ASN with specific time resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a48adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_asn_stats_per_period(root_folder, asn_map, include_asns=None, exclude_asns=None,\n",
    "                               time_range=None, aggfreq_s=None, tz=None, lan_subnets=None, \n",
    "                               specification_kwargs={}, report_freq=1440):\n",
    "    asn_stats = dict()\n",
    "    subnet_enum_state = {\"next_num\": 0}\n",
    "    \n",
    "    nfiles = 0\n",
    "    nentries = 0\n",
    "    \n",
    "    if include_asns is not None:\n",
    "        include_asns = set(include_asns)\n",
    "    if exclude_asns is not None:\n",
    "        exclude_asns = set(exclude_asns)\n",
    "    if time_range is not None:\n",
    "        time_range = (datetime_to_timestamp(time_range[0], unit=\"ns\"), \n",
    "                      datetime_to_timestamp(time_range[1], unit=\"ns\"))\n",
    "    if lan_subnets is not None:\n",
    "        lan_subnets = [netify(net) for net in lan_subnets]\n",
    "    \n",
    "    files = get_epping_files(root_folder)\n",
    "    \n",
    "    for file in files:\n",
    "        entries = get_epping_json_entries(file, verify_specification=True, \n",
    "                                          **specification_kwargs)\n",
    "        \n",
    "        nbins = entries[0][\"bins\"]\n",
    "        \n",
    "        for entry in entries:\n",
    "            if classify_epping_entry_type(entry) != \"subnet_stats\":\n",
    "                continue\n",
    "            \n",
    "            asn = asn_map.get(entry[\"ip_prefix\"], -3)\n",
    "            if include_asns is not None and asn not in include_asns:\n",
    "                continue\n",
    "            if exclude_asns is not None and asn in exclude_asns:\n",
    "                continue\n",
    "            \n",
    "            t = entry[\"timestamp\"]            \n",
    "            if (time_range is not None \n",
    "                and (t < time_range[0] or t > time_range[1])):\n",
    "                continue\n",
    "                \n",
    "            if aggfreq_s is not None:\n",
    "                t = int_floor(t, aggfreq_s * 1000000000)\n",
    "            \n",
    "            key = str(asn) + \"-\" + str(t)\n",
    "            if key in asn_stats:\n",
    "                _update_subnet_entry(asn_stats[key], entry)\n",
    "            else:\n",
    "                asn_stats[key] = _new_subnet_entry(entry, subnet_enum_state, \n",
    "                                                   lan_subnets, nbins)\n",
    "            \n",
    "            nentries += 1\n",
    "        \n",
    "        nfiles += 1\n",
    "        \n",
    "        if (report_freq > 0):\n",
    "            if (nfiles % report_freq == 0 or nfiles == len(files)):\n",
    "                print(\"{}: Parsed {}/{} files, containing {} entries\".format(\n",
    "                    datetime.datetime.now(), nfiles, len(files), nentries))\n",
    "    \n",
    "    return asn_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27aea454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-computed list of the top 100 ASNs with most downloaded bytes in our dataset\n",
    "with open(\"data/processed/top100asn_list.json\", \"rt\") as infile:\n",
    "    top100asn = json.load(infile)\n",
    "    \n",
    "# Use a pre-processed map of every /24 subnet -> ASN in our dataset\n",
    "# instead of using pyasn to speedup ASN lookups\n",
    "with open(\"data/processed/subnet_to_asn_map.json\", \"rt\") as infile:\n",
    "    asn_map = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e4665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc4f12fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-27 12:43:54.721041: Parsed 1440/46522 files, containing 9563444 entries\n",
      "2024-08-27 12:46:14.799966: Parsed 2880/46522 files, containing 18824260 entries\n",
      "2024-08-27 12:48:50.961759: Parsed 4320/46522 files, containing 28407439 entries\n",
      "2024-08-27 12:51:28.327399: Parsed 5760/46522 files, containing 37791179 entries\n",
      "2024-08-27 12:54:14.723432: Parsed 7200/46522 files, containing 47403567 entries\n",
      "2024-08-27 12:57:11.124007: Parsed 8640/46522 files, containing 57398903 entries\n",
      "2024-08-27 13:00:12.615331: Parsed 10080/46522 files, containing 67548860 entries\n",
      "2024-08-27 13:03:12.494616: Parsed 11520/46522 files, containing 77498873 entries\n",
      "2024-08-27 13:06:08.288552: Parsed 12960/46522 files, containing 87070573 entries\n",
      "2024-08-27 13:08:35.544540: Parsed 14400/46522 files, containing 95022299 entries\n",
      "2024-08-27 13:11:40.597842: Parsed 15840/46522 files, containing 104779737 entries\n",
      "2024-08-27 13:14:52.048099: Parsed 17280/46522 files, containing 114917587 entries\n",
      "2024-08-27 13:18:00.501724: Parsed 18720/46522 files, containing 124805341 entries\n",
      "2024-08-27 13:21:15.762355: Parsed 20160/46522 files, containing 135033621 entries\n",
      "2024-08-27 13:24:26.278832: Parsed 21600/46522 files, containing 144922235 entries\n",
      "2024-08-27 13:27:33.303598: Parsed 23040/46522 files, containing 154610943 entries\n",
      "2024-08-27 13:30:34.851683: Parsed 24480/46522 files, containing 163974057 entries\n",
      "2024-08-27 13:33:43.986964: Parsed 25920/46522 files, containing 173609703 entries\n",
      "2024-08-27 13:36:48.329011: Parsed 27360/46522 files, containing 183044617 entries\n",
      "2024-08-27 13:40:02.738779: Parsed 28800/46522 files, containing 192899573 entries\n",
      "2024-08-27 13:43:23.783932: Parsed 30240/46522 files, containing 203117898 entries\n",
      "2024-08-27 13:46:46.904692: Parsed 31680/46522 files, containing 213400148 entries\n",
      "2024-08-27 13:50:05.168332: Parsed 33120/46522 files, containing 223576355 entries\n",
      "2024-08-27 13:53:22.929131: Parsed 34560/46522 files, containing 233670915 entries\n",
      "2024-08-27 13:56:40.515149: Parsed 36000/46522 files, containing 243757627 entries\n",
      "2024-08-27 14:00:06.742053: Parsed 37440/46522 files, containing 254219294 entries\n",
      "2024-08-27 14:03:25.961146: Parsed 38880/46522 files, containing 264328170 entries\n",
      "2024-08-27 14:06:53.400819: Parsed 40320/46522 files, containing 274738439 entries\n",
      "2024-08-27 14:10:12.038120: Parsed 41760/46522 files, containing 284894868 entries\n",
      "2024-08-27 14:13:30.054105: Parsed 43200/46522 files, containing 294963534 entries\n",
      "2024-08-27 14:16:39.031457: Parsed 44640/46522 files, containing 304604421 entries\n",
      "2024-08-27 14:19:55.927698: Parsed 46080/46522 files, containing 314472321 entries\n",
      "2024-08-27 14:20:52.022076: Parsed 46522/46522 files, containing 317222387 entries\n",
      "CPU times: user 1h 38min 25s, sys: 10.2 s, total: 1h 38min 35s\n",
      "Wall time: 1h 39min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "asn_stats = parse_asn_stats_per_period(\"data/raw/\", asn_map, \n",
    "                                       include_asns=top100asn, \n",
    "                                       aggfreq_s=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5a4416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.9 s, sys: 228 ms, total: 45.1 s\n",
      "Wall time: 45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"data/processed/top100_asn_stats_10min.json\", \"wt\") as ofile:\n",
    "    json.dump(asn_stats, ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e567ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7a6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f15b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
