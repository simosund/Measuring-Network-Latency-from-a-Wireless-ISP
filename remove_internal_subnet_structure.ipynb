{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import ipaddress\n",
    "import re\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from util import *\n",
    "from histogram_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6c7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get json dumps to format floats the same as epping (%g format)\n",
    "# from https://stackoverflow.com/a/1733105\n",
    "class PrettyFloat(float):\n",
    "    def __repr__(self):\n",
    "        return '%g' % self\n",
    "    \n",
    "def pretty_floats(obj):\n",
    "    if isinstance(obj, float):\n",
    "        return PrettyFloat(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return dict((k, pretty_floats(v)) for k, v in obj.items())\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return list(map(pretty_floats, obj))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7aa1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_date(datestr):\n",
    "    return re.match(\"^\\d{4}-\\d{2}-\\d{2}$\", datestr) is not None\n",
    "\n",
    "def is_epping_filename(filename):\n",
    "    return re.match(\"^pping\\..*\\.json.\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}[\\+\\-]\\d{2}:\\d{2}(\\.gz)?$\", \n",
    "                    filename) is not None\n",
    "\n",
    "def get_epping_files(root_folder):\n",
    "    files = []\n",
    "    for datedir in os.scandir(root_folder):\n",
    "        if not (datedir.is_dir() and is_valid_date(datedir.name)):\n",
    "            continue\n",
    "        \n",
    "        for gzfile in os.scandir(datedir.path):\n",
    "            if gzfile.is_file() and is_epping_filename(gzfile.name):\n",
    "                files.append(gzfile.path)\n",
    "    \n",
    "    return sorted(files)\n",
    "\n",
    "def classify_epping_entry_type(entry):\n",
    "    if \"aggregation_interval_ns\" in entry:\n",
    "        return \"specification\"\n",
    "    \n",
    "    if \"ip_prefix\" in entry:\n",
    "        return \"subnet_stats\"\n",
    "    \n",
    "    if \"protocol_counters\" in entry:\n",
    "        return \"global_counters\"\n",
    "    \n",
    "    return \"unknown\"\n",
    "\n",
    "def verify_epping_specification(json_agg_format, nbins=250, bin_width_ms=4, \n",
    "                                agg_interval_s=10, ipv4_prefix_len=24, \n",
    "                                ipv6_prefix_len=48):\n",
    "    fields = {\"bins\": nbins,\n",
    "              \"bin_width_ns\": bin_width_ms * 1e6,\n",
    "              \"aggregation_interval_ns\": agg_interval_s * 1e9,\n",
    "              \"ipv4_prefix_len\": ipv4_prefix_len,\n",
    "              \"ipv6_prefix_len\": ipv6_prefix_len}\n",
    "    \n",
    "    for field, expected in fields.items():\n",
    "        if json_agg_format[field] != expected:\n",
    "            raise ValueError(\"{} is {}, expected {}\".format(\n",
    "                field, json_agg_format[field], expected))\n",
    "    return True\n",
    "\n",
    "def get_epping_json_entries(filename, verify_specification=True, **kwargs):\n",
    "    with gzip.open(filename) as jfile:\n",
    "        entries = json.load(jfile)\n",
    "    \n",
    "    if not classify_epping_entry_type(entries[0]) == \"specification\":\n",
    "        raise ValueError(\"{} does not start with a specification entry\".format(filename))\n",
    "    \n",
    "    for entry in entries[1:]:\n",
    "        if classify_epping_entry_type(entry) == \"specification\":\n",
    "            raise ValueError(\"{} contains multiple specification entires\".format(filename))\n",
    "    \n",
    "    if verify_specification:\n",
    "        verify_epping_specification(entries[0], **kwargs)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def is_in_subnets(subnet, include_subnets):\n",
    "    subnet = netify(subnet)\n",
    "    include_subnets = [netify(net) for net in include_subnets]\n",
    "    \n",
    "    for net in include_subnets:\n",
    "        if subnet.version == net.version and subnet.subnet_of(net):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251196df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_rxtx_stats(dst, src):\n",
    "    for traf_cat in src.keys():\n",
    "        if traf_cat in dst.keys():\n",
    "            dst[traf_cat][\"packets\"] += src[traf_cat][\"packets\"]\n",
    "            dst[traf_cat][\"bytes\"] += src[traf_cat][\"bytes\"]\n",
    "        else:\n",
    "            dst[traf_cat] = src[traf_cat].copy()\n",
    "    \n",
    "    return dst\n",
    "\n",
    "def add_to_entry(dst, src, bins):\n",
    "    if dst[\"timestamp\"] != src[\"timestamp\"]:\n",
    "        raise ValueError(\"dst and src from different times, should not be merged!\")\n",
    "    \n",
    "    add_to_rxtx_stats(dst[\"rx_stats\"], src[\"rx_stats\"])\n",
    "    add_to_rxtx_stats(dst[\"tx_stats\"], src[\"tx_stats\"])\n",
    "    \n",
    "    if \"count_rtt\" in src and src[\"count_rtt\"] > 0:\n",
    "        dst[\"min_rtt\"] = min(dst.get(\"min_rtt\", 1e100), src[\"min_rtt\"])\n",
    "        dst[\"max_rtt\"] = max(dst.get(\"max_rtt\", 0), src[\"max_rtt\"])\n",
    "        \n",
    "        hist = sum_histograms([dst.get(\"histogram\", []), src[\"histogram\"]])\n",
    "        bins = bins[:len(hist) + 1]\n",
    "        dst[\"histogram\"] = hist\n",
    "        \n",
    "        dst[\"count_rtt\"] = int(bincount_count(bins, hist))\n",
    "        dst[\"mean_rtt\"] = float(bincount_mean(bins, hist))\n",
    "        dst[\"median_rtt\"] = float(bincount_median(bins, hist))\n",
    "        dst[\"p95_rtt\"] = float(bincount_quantile(bins, hist, 0.95))\n",
    "    \n",
    "    return dst\n",
    "\n",
    "def merge_subnet_entries(json_entries, subnets, replacement_subnet=\"100.64.0.0/24\", \n",
    "                         del_subnets=None):\n",
    "    curr_t = 0\n",
    "    merged_entry = None\n",
    "    del_idx = []\n",
    "    \n",
    "    subnets = [netify(subnet) for subnet in subnets]\n",
    "    if del_subnets is not None:\n",
    "        del_subnets = [netify(subnet) for subnet in del_subnets]\n",
    "    \n",
    "    spec = json_entries[0]\n",
    "    if classify_epping_entry_type(spec) != \"specification\":\n",
    "        raise ValueError(\"Missing initial specification entry - cannot get histogram config\")\n",
    "    \n",
    "    nbins = spec[\"bins\"]\n",
    "    bin_width = spec[\"bin_width_ns\"]\n",
    "    bins = np.arange(0, nbins * bin_width + 1, bin_width)\n",
    "    \n",
    "    for i, entry in enumerate(json_entries):\n",
    "        if classify_epping_entry_type(entry) != \"subnet_stats\":\n",
    "            continue\n",
    "        if del_subnets is not None and is_in_subnets(entry[\"ip_prefix\"], \n",
    "                                                     del_subnets):\n",
    "            del_idx.append(i)\n",
    "            continue\n",
    "        if not is_in_subnets(entry[\"ip_prefix\"], subnets):\n",
    "            continue\n",
    "        \n",
    "        if entry[\"timestamp\"] != curr_t:\n",
    "            curr_t = entry[\"timestamp\"]\n",
    "            merged_entry = entry\n",
    "            entry[\"ip_prefix\"] = replacement_subnet\n",
    "        else:\n",
    "            add_to_entry(merged_entry, entry, bins)\n",
    "            del_idx.append(i)\n",
    "    \n",
    "    json_entries[:] = [entry for idx, entry in enumerate(json_entries) if idx not in del_idx]\n",
    "    return json_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dad39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3dbde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merges together all customer LAN subnets into a single 100.64.0.0/24 entry to\n",
    "avoid leaking information about the ISPs internal network.\n",
    "Also removes some subnet used for internal network management for similar reasons.\n",
    "'''\n",
    "def clean_jsondata(root_folder, new_root, merge_subnets=[], subnet_replacement=\"100.64.0.0/24\", \n",
    "                   del_subnets=[], specification_kwargs={}, report_freq=1440, compresslevel=6):\n",
    "    nfiles = 0\n",
    "    nentries = 0\n",
    "    nmergedel = 0\n",
    "    \n",
    "    files = get_epping_files(root_folder)\n",
    "    for file in files:\n",
    "        entries = get_epping_json_entries(file)\n",
    "        prelen = len(entries)\n",
    "        \n",
    "        entries = merge_subnet_entries(entries, merge_subnets, \n",
    "                                       replacement_subnet=subnet_replacement, \n",
    "                                       del_subnets=del_subnets)\n",
    "        \n",
    "        nentries += prelen\n",
    "        nmergedel += prelen - len(entries)\n",
    "        \n",
    "        save_file = os.path.join(new_root, os.path.relpath(file, root_folder))\n",
    "        os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "        if os.path.exists(save_file):\n",
    "            raise FileExistsError(\"{} already exists\".format(save_file))\n",
    "        \n",
    "        with gzip.open(save_file, \"wt\", compresslevel=compresslevel) as outfile:\n",
    "            json.dump(entries, outfile, separators=(\",\",\":\"))\n",
    "            \n",
    "        nfiles += 1\n",
    "        \n",
    "        if (report_freq > 0 and (nfiles % report_freq == 0 or nfiles == len(files))):\n",
    "            print(\"{}: Parsed {}/{} files, containing {} entries ({} merged or dropped)\".format(\n",
    "                datetime.datetime.now(), nfiles, len(files), nentries, nmergedel))\n",
    "    \n",
    "    return nfiles, nentries, nmergedel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df786640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4669db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lan_subnets_to_merge = [# Removed list of internal subnets used by ISP\n",
    "                            ]\n",
    "\n",
    "lan_subnets_to_remove = [# Removed list of internal subnets used by ISP\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f42f1d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 17:14:16.097685: Parsed 1440/46522 files, containing 10703453 entries (225462 merged or dropped)\n",
      "2024-08-21 17:28:04.579010: Parsed 2880/46522 files, containing 21131617 entries (443351 merged or dropped)\n",
      "2024-08-21 17:42:23.515666: Parsed 4320/46522 files, containing 31948463 entries (663273 merged or dropped)\n",
      "2024-08-21 17:56:06.102021: Parsed 5760/46522 files, containing 42486616 entries (885994 merged or dropped)\n",
      "2024-08-21 18:10:17.679646: Parsed 7200/46522 files, containing 53317821 entries (1112148 merged or dropped)\n",
      "2024-08-21 18:25:14.759356: Parsed 8640/46522 files, containing 64566341 entries (1345935 merged or dropped)\n",
      "2024-08-21 18:40:16.116448: Parsed 10080/46522 files, containing 75980364 entries (1588208 merged or dropped)\n",
      "2024-08-21 18:55:09.230121: Parsed 11520/46522 files, containing 87200788 entries (1818702 merged or dropped)\n",
      "2024-08-21 19:09:39.660753: Parsed 12960/46522 files, containing 98021512 entries (2028267 merged or dropped)\n",
      "2024-08-21 19:21:32.050828: Parsed 14400/46522 files, containing 106941488 entries (2206183 merged or dropped)\n",
      "2024-08-21 19:36:23.551088: Parsed 15840/46522 files, containing 117919865 entries (2423105 merged or dropped)\n",
      "2024-08-21 19:51:26.659842: Parsed 17280/46522 files, containing 129321813 entries (2643225 merged or dropped)\n",
      "2024-08-21 20:06:04.642279: Parsed 18720/46522 files, containing 140464453 entries (2865677 merged or dropped)\n",
      "2024-08-21 20:21:12.973545: Parsed 20160/46522 files, containing 151995085 entries (3091490 merged or dropped)\n",
      "2024-08-21 20:35:54.428130: Parsed 21600/46522 files, containing 163203760 entries (3311205 merged or dropped)\n",
      "2024-08-21 20:50:22.254483: Parsed 23040/46522 files, containing 174171777 entries (3530235 merged or dropped)\n",
      "2024-08-21 21:04:19.095872: Parsed 24480/46522 files, containing 184794583 entries (3756166 merged or dropped)\n",
      "2024-08-21 21:18:55.068397: Parsed 25920/46522 files, containing 195684495 entries (3977712 merged or dropped)\n",
      "2024-08-21 21:33:02.358517: Parsed 27360/46522 files, containing 206355938 entries (4199296 merged or dropped)\n",
      "2024-08-21 21:47:44.773112: Parsed 28800/46522 files, containing 217533470 entries (4424899 merged or dropped)\n",
      "2024-08-21 22:03:00.770208: Parsed 30240/46522 files, containing 229107225 entries (4658047 merged or dropped)\n",
      "2024-08-21 22:18:14.033392: Parsed 31680/46522 files, containing 240718213 entries (4880303 merged or dropped)\n",
      "2024-08-21 22:33:23.430673: Parsed 33120/46522 files, containing 252211513 entries (5102437 merged or dropped)\n",
      "2024-08-21 22:48:15.923106: Parsed 34560/46522 files, containing 263595469 entries (5325947 merged or dropped)\n",
      "2024-08-21 23:03:11.987414: Parsed 36000/46522 files, containing 274937215 entries (5547151 merged or dropped)\n",
      "2024-08-21 23:18:45.157611: Parsed 37440/46522 files, containing 286737244 entries (5768762 merged or dropped)\n",
      "2024-08-21 23:33:42.490693: Parsed 38880/46522 files, containing 298133097 entries (5991584 merged or dropped)\n",
      "2024-08-21 23:49:10.337671: Parsed 40320/46522 files, containing 309855442 entries (6230495 merged or dropped)\n",
      "2024-08-22 00:04:05.255737: Parsed 41760/46522 files, containing 321297242 entries (6456713 merged or dropped)\n",
      "2024-08-22 00:18:53.841812: Parsed 43200/46522 files, containing 332661445 entries (6685556 merged or dropped)\n",
      "2024-08-22 00:33:05.274341: Parsed 44640/46522 files, containing 343495920 entries (6912560 merged or dropped)\n",
      "2024-08-22 00:47:53.401402: Parsed 46080/46522 files, containing 354709668 entries (7140710 merged or dropped)\n",
      "2024-08-22 00:52:03.038970: Parsed 46522/46522 files, containing 357907307 entries (7211260 merged or dropped)\n",
      "CPU times: user 7h 50min 10s, sys: 30.5 s, total: 7h 50min 40s\n",
      "Wall time: 7h 52min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46522, 357907307, 7211260)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clean_jsondata(\"data/original/raw\", \n",
    "               \"data/raw\",\n",
    "               merge_subnets=lan_subnets_to_merge,\n",
    "               subnet_replacement=\"100.64.0.0/24\",\n",
    "               del_subnets=lan_subnets_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00e833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a30ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
